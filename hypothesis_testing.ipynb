{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this problem you will be analysing polling data. Specifically, the data that you will be working with has been obtained from http://projects.fivethirtyeight.com/2016-election-forecast/. If you're interested, you can go through some interesting visualizations and analyses they have performed on top of this data.\n",
    "\n",
    "# Polling Data\n",
    "The polling data consists of several polls (uniquely identified by `poll_id`) conducted by pollsters. Each pollster is associated with a `poll_wt` and `grade`, which reflect the confidence in the poll results conducted by them. The raw poll counts reflect the actual results of the poll, while the adjusted polls are the forecasts made by http://projects.fivethirtyeight.com using one of its three models (this is the `type` column). For more details on attributes, see [this](http://fivethirtyeight.com/features/a-users-guide-to-fivethirtyeights-2016-general-election-forecast/) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Load data [2pts]\n",
    "You will load the data from CSV file into a pandas dataframe, following the specifications given below.\n",
    "\n",
    "### Specifications\n",
    "1. State must be one of the 50 states (or U.S.), without being further subdivided (e.g., see Maine).\n",
    "2. Some polls may not have the option of Johnson or McMullin, so their raw and adjusted poll counts would be empty. We need treat these as zeros.\n",
    "3. Assign grade 'E' to any ungraded pollster.\n",
    "4. Create a new column called 'adj_poll_weight' which takes into account both the poll_wt and the sample size. This is will be used as the *adjusted weight* for each poll.\n",
    "\n",
    " adj_poll_weight = poll_wt * $log_{10}$(samplesize)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9525\n",
      "type                        object\n",
      "state                       object\n",
      "startdate           datetime64[ns]\n",
      "enddate             datetime64[ns]\n",
      "pollster                    object\n",
      "grade                       object\n",
      "samplesize                 float64\n",
      "population                  object\n",
      "poll_wt                    float64\n",
      "rawpoll_clinton            float64\n",
      "rawpoll_trump              float64\n",
      "rawpoll_johnson            float64\n",
      "rawpoll_mcmullin           float64\n",
      "adjpoll_clinton            float64\n",
      "adjpoll_trump              float64\n",
      "adjpoll_johnson            float64\n",
      "adjpoll_mcmullin           float64\n",
      "url                         object\n",
      "poll_id                      int64\n",
      "question_id                  int64\n",
      "createddate         datetime64[ns]\n",
      "adj_poll_weight            float64\n",
      "dtype: object\n",
      "         type    state  startdate    enddate  \\\n",
      "0  polls-plus     U.S. 2016-10-20 2016-10-24   \n",
      "1  polls-plus     U.S. 2016-10-20 2016-10-25   \n",
      "2  polls-plus  Florida 2016-10-20 2016-10-24   \n",
      "3  polls-plus     U.S. 2016-10-22 2016-10-25   \n",
      "4  polls-plus     U.S. 2016-10-25 2016-10-27   \n",
      "\n",
      "                                            pollster grade  samplesize  \\\n",
      "0                            Google Consumer Surveys     B     21240.0   \n",
      "1                                Pew Research Center    B+      2120.0   \n",
      "2                                          SurveyUSA     A      1251.0   \n",
      "3  Fox News/Anderson Robbins Research/Shaw & Comp...     A      1221.0   \n",
      "4                           ABC News/Washington Post    A+       956.0   \n",
      "\n",
      "  population   poll_wt  rawpoll_clinton       ...         rawpoll_mcmullin  \\\n",
      "0         lv  5.237220            38.54       ...                      0.0   \n",
      "1         rv  3.623270            46.00       ...                      0.0   \n",
      "2         lv  3.584933            48.00       ...                      0.0   \n",
      "3         lv  3.561260            44.00       ...                      0.0   \n",
      "4         lv  3.471576            47.00       ...                      0.0   \n",
      "\n",
      "   adjpoll_clinton  adjpoll_trump  adjpoll_johnson  adjpoll_mcmullin  \\\n",
      "0         43.46984       39.98077         5.426960               0.0   \n",
      "1         45.46572       41.28611         3.960040               0.0   \n",
      "2         46.66093       44.43937         2.152259               0.0   \n",
      "3         44.91556       41.45449         6.522821               0.0   \n",
      "4         45.13807       43.87921         4.487876               0.0   \n",
      "\n",
      "                                                 url  poll_id question_id  \\\n",
      "0  https://datastudio.google.com/u/0/#/org//repor...    47407       74188   \n",
      "1  http://www.people-press.org/2016/10/27/as-elec...    47616       74519   \n",
      "2  http://www.baynews9.com/content/news/baynews9/...    47465       74252   \n",
      "3  http://www.foxnews.com/politics/interactive/20...    47542       74365   \n",
      "4  http://www.langerresearch.com/wp-content/uploa...    47711       74693   \n",
      "\n",
      "   createddate  adj_poll_weight  \n",
      "0   2016-10-25        22.662260  \n",
      "1   2016-10-27        12.052213  \n",
      "2   2016-10-25        11.103460  \n",
      "3   2016-10-26        10.992597  \n",
      "4   2016-10-29        10.346886  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\" Loads data from the input CSV file and processes it as mentioned above\n",
    "    Inputs:\n",
    "        file_name (str): file name\n",
    "    Outputs:\n",
    "        pd.DataFrame: processed data frame containing polls\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_name)\n",
    "    #1. Fix divisions\n",
    "    data = data.replace(to_replace = ['Maine CD-1','Maine CD-2'],value = 'Maine')\n",
    "    data = data.replace(to_replace = ['Nebraska CD-1','Nebraska CD-2','Nebraska CD-3',],value = 'Nebraska')\n",
    "    \n",
    "    #Change to date objects\n",
    "    data['startdate'] = pd.to_datetime(data['startdate'], format='%m/%d/%y')\n",
    "    data['enddate'] = pd.to_datetime(data['enddate'], format='%m/%d/%y')\n",
    "    data['createddate'] = pd.to_datetime(data['createddate'], format='%m/%d/%y')\n",
    "    \n",
    "    #2. Replace option of Johnson or McMullin so their raw and adjusted poll counts\n",
    "    data['adjpoll_mcmullin'] = data['adjpoll_mcmullin'].fillna(0)\n",
    "    data['rawpoll_mcmullin'] = data['rawpoll_mcmullin'].fillna(0)\n",
    "    \n",
    "    data['adjpoll_johnson'] = data['adjpoll_johnson'].fillna(0)\n",
    "    data['rawpoll_johnson'] = data['rawpoll_johnson'].fillna(0)\n",
    "    \n",
    "    #3. Assign grade 'E' to any ungraded pollster\n",
    "    data['grade'] = data['grade'].fillna('E')\n",
    "    #4. New column \n",
    "    data = data.dropna()\n",
    "#     data = data[data.samplesize != 0]\n",
    "#     data = data[data.poll_wt != 0]\n",
    "    \n",
    "    data['adj_poll_weight'] = data['poll_wt'] * np.log10(data['samplesize'])\n",
    "    print data.shape[0]\n",
    "#     print data.adjpoll_mcmullin.unique()\n",
    "    return data\n",
    "\n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "df = load_data('polls.csv')\n",
    "print df.dtypes\n",
    "print df.head()\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reference yields the following output:\n",
    "\n",
    "```python\n",
    ">>> df.dtypes\n",
    "type                        object\n",
    "state                       object\n",
    "startdate           datetime64[ns]\n",
    "enddate             datetime64[ns]\n",
    "pollster                    object\n",
    "grade                       object\n",
    "samplesize                 float64\n",
    "population                  object\n",
    "poll_wt                    float64\n",
    "rawpoll_clinton            float64\n",
    "rawpoll_trump              float64\n",
    "rawpoll_johnson            float64\n",
    "rawpoll_mcmullin           float64\n",
    "adjpoll_clinton            float64\n",
    "adjpoll_trump              float64\n",
    "adjpoll_johnson            float64\n",
    "adjpoll_mcmullin           float64\n",
    "url                         object\n",
    "poll_id                      int64\n",
    "question_id                  int64\n",
    "createddate         datetime64[ns]\n",
    "adj_poll_weight            float64\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> df.head()\n",
    "         type    state  startdate    enddate  \\\n",
    "0  polls-plus     U.S. 2016-10-20 2016-10-24   \n",
    "1  polls-plus     U.S. 2016-10-20 2016-10-25   \n",
    "2  polls-plus  Florida 2016-10-20 2016-10-24   \n",
    "3  polls-plus     U.S. 2016-10-22 2016-10-25   \n",
    "4  polls-plus     U.S. 2016-10-25 2016-10-27   \n",
    "\n",
    "                                            pollster grade  samplesize  \\\n",
    "0                            Google Consumer Surveys     B     21240.0   \n",
    "1                                Pew Research Center    B+      2120.0   \n",
    "2                                          SurveyUSA     A      1251.0   \n",
    "3  Fox News/Anderson Robbins Research/Shaw & Comp...     A      1221.0   \n",
    "4                           ABC News/Washington Post    A+       956.0   \n",
    "\n",
    "  population   poll_wt  rawpoll_clinton       ...         rawpoll_mcmullin  \\\n",
    "0         lv  5.237220            38.54       ...                      0.0   \n",
    "1         rv  3.623270            46.00       ...                      0.0   \n",
    "2         lv  3.584933            48.00       ...                      0.0   \n",
    "3         lv  3.561260            44.00       ...                      0.0   \n",
    "4         lv  3.471576            47.00       ...                      0.0   \n",
    "\n",
    "   adjpoll_clinton  adjpoll_trump  adjpoll_johnson  adjpoll_mcmullin  \\\n",
    "0         43.46984       39.98077         5.426960               0.0   \n",
    "1         45.46572       41.28611         3.960040               0.0   \n",
    "2         46.66093       44.43937         2.152259               0.0   \n",
    "3         44.91556       41.45449         6.522821               0.0   \n",
    "4         45.13807       43.87921         4.487876               0.0   \n",
    "\n",
    "                                                 url  poll_id question_id  \\\n",
    "0  https://datastudio.google.com/u/0/#/org//repor...    47407       74188   \n",
    "1  http://www.people-press.org/2016/10/27/as-elec...    47616       74519   \n",
    "2  http://www.baynews9.com/content/news/baynews9/...    47465       74252   \n",
    "3  http://www.foxnews.com/politics/interactive/20...    47542       74365   \n",
    "4  http://www.langerresearch.com/wp-content/uploa...    47711       74693   \n",
    "\n",
    "   createddate  adj_poll_weight  \n",
    "0   2016-10-25        22.662260  \n",
    "1   2016-10-27        12.052213  \n",
    "2   2016-10-25        11.103460  \n",
    "3   2016-10-26        10.992597  \n",
    "4   2016-10-29        10.346886  \n",
    "\n",
    "[5 rows x 22 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Statistics\n",
    "\n",
    "Recall from class slides that the sample mean and variance for a list of observations $x_i$ for $i=1,\\ldots,m$ are given by \n",
    "$$ \\bar{x} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$\n",
    "\n",
    "$$ s^2 = \\frac{1}{m-1} \\sum_{i=1}^{m} (x_i - \\bar{x})^2 $$\n",
    "\n",
    "## Q2. Sample mean and variance [4pts]\n",
    "These unweighted observations can be treated as weighted observations, each with weight 1. Your first task is to extend sample mean and variance to weighted observations and implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.33333333333\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "def sample_mean(x, w):\n",
    "    \"\"\" Estimates weighted mean of data\n",
    "    Inputs:\n",
    "        x (array-like): array of data points/observations\n",
    "        w (array-like): array of weights for these observations\n",
    "    Outputs:\n",
    "        (float) - weighted mean of data\n",
    "    \"\"\"\n",
    "    return np.sum((w*x))*(1.0/np.sum(w))\n",
    "\n",
    "def sample_variance(x, w):\n",
    "    \"\"\" Estimates sample variance of data\n",
    "    Inputs:\n",
    "        x (array-like): array of data points/observations\n",
    "        w (array-like): array of weights for these observations\n",
    "    Outputs:\n",
    "        (float) - sample variance of data (according to the given weights)\n",
    "    \"\"\"\n",
    "    inner = np.square(x - sample_mean(x,w))\n",
    "#     print inner\n",
    "    left = np.sum(w*inner)\n",
    "    s = (1.0/((np.sum(w)) - 1))\n",
    "    return left*s\n",
    "    \n",
    "#AUTOLAB_IGNORE_START\n",
    "print sample_mean(np.arange(10), np.arange(10))\n",
    "print sample_variance(np.arange(10), np.arange(10))\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation yields:\n",
    "\n",
    "```python\n",
    ">>> sample_mean(np.arange(10), np.arange(10))\n",
    "6.33333333333\n",
    ">>> sample_variance(np.arange(10), np.arange(10))\n",
    "5.0\n",
    "```\n",
    "\n",
    "## Q3. Confidence Interval [4pts]\n",
    "Using the computation of sample mean and variance above, implement the following function to compute the two-sided confidence interval of mean, using the T-statistic or normal distribution depending on the total weight of observations. Use the rule of thumb given in the slides (after adapting it for weighted samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.2459476124196693, 3.7540523875803307)\n",
      "(32.34667867181998, 33.65332132818002)\n"
     ]
    }
   ],
   "source": [
    "def two_sided_confidence_interval_of_mean(x, w, alpha=0.05):\n",
    "    \"\"\" Estimates confidence interval of mean of data using the Student's T distribution or normal distribution.\n",
    "    Inputs:\n",
    "        x (array-like): array of data points/observations\n",
    "        w (array-like): array of weights for these observations\n",
    "        alpha (float): confidence level\n",
    "    Outputs:\n",
    "        (float, float) - lower and upper limit of the confidence interval of mean of data \n",
    "                        (according to the given weights)\n",
    "    \"\"\"\n",
    "\n",
    "    if (sum(w) < 30):\n",
    "        CI = lambda s,m,a : float(np.sqrt(s/float(m))) * st.t(m-1).ppf(1.0-a/2.0)\n",
    "        right = sample_mean(x,w) + CI(sample_variance(x,w),np.sum(w),alpha)\n",
    "        left = sample_mean(x,w) - CI(sample_variance(x,w),np.sum(w),alpha)\n",
    "        return (left,right)\n",
    "    if (sum(w) >= 30):\n",
    "        CI = lambda s,m,a : float(np.sqrt(s/float(m))) * st.norm().ppf(1.0-a/2.0)\n",
    "        right = sample_mean(x,w) + CI(sample_variance(x,w),np.sum(w),alpha)\n",
    "        left = sample_mean(x,w) - CI(sample_variance(x,w),np.sum(w),alpha)\n",
    "        return (left,right)        \n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "print two_sided_confidence_interval_of_mean(np.arange(5), np.arange(5))\n",
    "print two_sided_confidence_interval_of_mean(np.arange(50), np.arange(50))\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implemenation yields:\n",
    "\n",
    "```python\n",
    ">>> print two_sided_confidence_interval_of_mean(np.arange(5), np.arange(5))\n",
    "(2.2459476124196693, 3.7540523875803307)\n",
    ">>> two_sided_confidence_interval_of_mean(np.arange(50), np.arange(50))\n",
    "(32.34667867181998, 33.65332132818002)\n",
    "```\n",
    "\n",
    "## Q4. Swing states [7pts]\n",
    "In this part, you will first implement a function to compute the confidence interval of raw poll of all candidates in a given state. In doing so, make sure to take into account a given poll (identified uniquely by the `poll_id`) exactly once. (A single poll occurs under each `type` once, with the same value of raw polls, but different value of adjusted polls.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clinton': (45.034797316193895, 45.904042863492315), 'trump': (42.591138848195598, 43.480477966774188), 'johnson': (3.3155643710340765, 4.0095495564481176), 'mcmullin': (0.0, 0.0)}\n",
      "{'clinton': (41.414017061498271, 44.033327540603793), 'trump': (35.861329660838202, 38.177661364411641), 'johnson': (5.3307907923279281, 7.8243833448439855), 'mcmullin': (0.0, 0.0)}\n"
     ]
    }
   ],
   "source": [
    "def poll_confidence_intervals(df, state='Florida', alpha=0.05):\n",
    "    \"\"\" Estimates confidence intervals for raw polls of clinton, trump, johnson and mcmullin\n",
    "    Inputs:\n",
    "        df (pd.DataFrame) - data frame with polls data\n",
    "        state (str) - state for which confidence interval of mean has to be computed\n",
    "        alpha (float) - confidence level\n",
    "    Outputs:\n",
    "        dict: keys are candidate names and values are the confidence intervals (i.e., tuples of floats, \n",
    "                indicating the lower and upper bounds of the interval, respectively)\n",
    "    \"\"\"\n",
    "    data = df[df.state == state]\n",
    "    data = data.drop_duplicates(subset = 'poll_id')\n",
    "    cd = {}\n",
    "    cd['mcmullin'] = two_sided_confidence_interval_of_mean(data.rawpoll_mcmullin, data.adj_poll_weight, alpha)\n",
    "    cd['clinton'] = two_sided_confidence_interval_of_mean(data.rawpoll_clinton, data.adj_poll_weight, alpha)\n",
    "    cd['johnson'] = two_sided_confidence_interval_of_mean(data.rawpoll_johnson, data.adj_poll_weight, alpha)\n",
    "    cd['trump'] = two_sided_confidence_interval_of_mean(data.rawpoll_trump, data.adj_poll_weight, alpha)\n",
    "\n",
    "    return cd\n",
    "    \n",
    "    #two_sided_confidence_interval_of_mean\n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "print poll_confidence_intervals(df, state='Florida', alpha=0.05)\n",
    "print poll_confidence_intervals(df, state='Maine', alpha=0.1)\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation yields:\n",
    "```python\n",
    ">>> print poll_confidence_intervals(df, state='Florida', alpha=0.05)\n",
    "{'clinton': (45.034797316193895, 45.904042863492315), 'trump': (42.591138848195598, 43.480477966774188), 'johnson': (3.3155643710340765, 4.0095495564481176), 'mcmullin': (0.0, 0.0)}\n",
    ">>> print poll_confidence_intervals(df, state='Maine', alpha=0.1)\n",
    "{'clinton': (41.414017061498278, 44.0333275406038), 'trump': (35.861329660838202, 38.177661364411641), 'johnson': (5.3307907923279281, 7.8243833448439855), 'mcmullin': (0.0, 0.0)}\n",
    "```\n",
    "\n",
    "Now, let us define **swing state** as a state (exclude U.S.) where the confidence intervals (at a specified confidence level) of the leading candidates (we will assume them to be `trump` and `clinton` in all states) overlap. Report the set of all swing states by filling the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Ohio', 'Arizona', 'Iowa'])\n"
     ]
    }
   ],
   "source": [
    "def swing_states(df, alpha=0.05):\n",
    "    \"\"\" Determines the set of swing states at a given confidence level\n",
    "    Inputs:\n",
    "        df (pd.DataFrame) - data frame with polls data\n",
    "        alpha (float) - confidence level\n",
    "    Outputs:\n",
    "        set(str) - set of swing states\n",
    "    \"\"\"\n",
    "    swing_states = []\n",
    "    for state in df.state.unique():\n",
    "        #Check if it overlaps\n",
    "        trump = poll_confidence_intervals(df, state, alpha)['trump']\n",
    "        clinton = poll_confidence_intervals(df, state, alpha)['clinton']\n",
    "        if (trump[0] > clinton[0] and trump[0] < clinton[1]) or (trump[1] > clinton[0] and trump[1] < clinton[0]):\n",
    "            swing_states.append(state)\n",
    "        elif (clinton[0] > trump[0] and clinton[0] < trump[1]) or (clinton[1] > trump[0] and clinton[1] < trump[0]):\n",
    "            swing_states.append(state)\n",
    "    return set(swing_states)\n",
    "    \n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "print swing_states(df, alpha=0.05)\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gives:\n",
    "```python\n",
    ">>>  swing_states(df, alpha=0.05)\n",
    "set(['Ohio', 'Arizona', 'Iowa'])\n",
    "```\n",
    "\n",
    "## Q5. Significant differences [8pts]\n",
    "The adjusted polls in data have been determined using three models:\n",
    "- *polls-plus*: What polls, the economy and historical data tell us about Nov. 8\n",
    "- *polls-only*: What polls alone tell us about Nov. 8\n",
    "- *now-cast*: Who would win the election if it were held today\n",
    "\n",
    "Is there a difference in the adjusted polls using the three models? For a given pair of models (`type1` and `type2`), take the null hypothesis to be that the adjusted polls by both types have the same (weighted) mean (for a given candidate), and the alternate hypothesis to be that they have different (weighted) means. Report the t- statistic and the p-value using the Welch's $t$-test from class slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285499952751\n",
      "0.775275432604\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "### VERY IMPORTANT M IS THE SUM OF WEIGHTS\n",
    "##############\n",
    "def difference_model(df, type1='polls-only', type2='now-cast', candidate='clinton'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        df (pd.DataFrame) - data frame with polls data\n",
    "        type1 (str) - model type 1\n",
    "        type2 (str) - model type 2\n",
    "        candidate (str) - candidate whose poll is being compared\n",
    "    Outputs:\n",
    "        float - p-value of hypothesis\n",
    "    \"\"\"\n",
    "    df1 = df[df.type == type1]\n",
    "    df2 = df[df.type == type2]\n",
    "    poll = \"adjpoll_\" + candidate\n",
    "    \n",
    "    xbar1 = sample_mean(df1[poll],df1['adj_poll_weight'])\n",
    "    xbar2 = sample_mean(df2[poll],df2['adj_poll_weight'])\n",
    "\n",
    "    s_var1 = sample_variance(df1[poll],df1['adj_poll_weight'])\n",
    "    s_var2 = sample_variance(df2[poll],df2['adj_poll_weight'])\n",
    "    \n",
    "    w1 = np.sum(df1['adj_poll_weight'])\n",
    "    w2 = np.sum(df2['adj_poll_weight'])\n",
    "    \n",
    "    t = (xbar1 - xbar2) / np.sqrt((s_var1/w1) + (s_var2/w2))\n",
    "    num = np.square(s_var1/w1 + s_var2/w2)\n",
    "    denom = (np.square(s_var1/w1)/(w1 - 1.0)) + (np.square(s_var2/w2)/(w2 - 1.0))\n",
    "    degrees = num/denom\n",
    "\n",
    "    t_dist = st.t(degrees)\n",
    "    p = 2*t_dist.cdf(-np.abs(t))\n",
    "    return p\n",
    "    \n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "print difference_model(df, type1='polls-only', type2='now-cast', candidate='clinton')\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gives a p-value of 0.663145169.\n",
    "\n",
    "Perform a similar hypothesis test on the *raw polls* (again, use each `poll_id` exactly once) using pollsters of different grades. For a given pair of grades (`grade1` and `grade2`), your null hypothesis is that the (weighted) mean of raw polls from pollsters of different grades is identical and your alternate hypothesis is that these are unequal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0307326598542\n"
     ]
    }
   ],
   "source": [
    "def difference_grade(df, grade1='A+', grade2='B+', candidate='clinton'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "    Outputs:\n",
    "        float - p-value of hypothesis\n",
    "    \"\"\"\n",
    "    df = df.drop_duplicates(subset = 'poll_id')\n",
    "    df1 = df[df.grade == grade1]\n",
    "    df2 = df[df.grade == grade2]\n",
    "    poll = \"rawpoll_\" + candidate\n",
    "    \n",
    "    xbar1 = sample_mean(df1[poll],df1['adj_poll_weight'])\n",
    "    xbar2 = sample_mean(df2[poll],df2['adj_poll_weight'])\n",
    "\n",
    "    s_var1 = sample_variance(df1[poll],df1['adj_poll_weight'])\n",
    "    s_var2 = sample_variance(df2[poll],df2['adj_poll_weight'])\n",
    "    \n",
    "    w1 = np.sum(df1['adj_poll_weight'])\n",
    "    w2 = np.sum(df2['adj_poll_weight'])\n",
    "    \n",
    "    t = (xbar1 - xbar2) / np.sqrt((s_var1/w1) + (s_var2/w2))\n",
    "    num = np.square(s_var1/w1 + s_var2/w2)\n",
    "    denom = (np.square(s_var1/w1)/(w1 - 1.0)) + (np.square(s_var2/w2)/(w2 - 1.0))\n",
    "    degrees = num/denom\n",
    "\n",
    "    t_dist = st.t(degrees)\n",
    "    p = 2*t_dist.cdf(-np.abs(t))\n",
    "    return p    \n",
    "\n",
    "#AUTOLAB_IGNORE_START\n",
    "print difference_grade(df, grade1='A+', grade2='B+', candidate='clinton')\n",
    "#AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gives a p-value of 0.0307300897182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
